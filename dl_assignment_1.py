# -*- coding: utf-8 -*-
"""DL_Assignment_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HUW7QEPPY_ZCcMvCHtCDttsTqJHoBvG-
"""

import numpy as np
import matplotlib.pyplot as plt
import wandb
from keras.datasets import fashion_mnist

# Initialize wandb
wandb.init(project="DA6401_Assignment1_ma23m011", name="sample-images-1")

# Load Fashion-MNIST dataset
(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()

# Define class labels for Fashion-MNIST
class_labels = [
    "T-shirt/top", "Trouser", "Pullover", "Dress", "Coat",
    "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"
]

# Get one example of each class
sample_images = []
sample_labels = []
for class_id in range(10):
    idx = np.where(y_train == class_id)[0][0]  # Find first occurrence of class_id
    sample_images.append(x_train[idx])
    sample_labels.append(class_labels[class_id])

# Log images to wandb
wandb.log({"Sample Images": [wandb.Image(img, caption=label) for img, label in zip(sample_images, sample_labels)]})

# Plot images in a 2x5 grid
fig, axes = plt.subplots(2, 5, figsize=(10, 5))
fig.suptitle("Sample Images from Fashion-MNIST", fontsize=14)

for i, ax in enumerate(axes.flat):
    ax.imshow(sample_images[i], cmap='gray')
    ax.set_title(sample_labels[i])
    ax.axis("off")

plt.show()

# Finish wandb run
wandb.finish()

# # Normalize the input images (scale pixel values to [0,1])
# x_train = x_train.reshape(x_train.shape[0], -1) / 255.0  # Flatten 28x28 images to 784-dim vector
# x_test = x_test.reshape(x_test.shape[0], -1) / 255.0

# # Convert labels to one-hot encoding
# def one_hot_encode(y, num_classes=10):
#     one_hot = np.zeros((y.size, num_classes))
#     one_hot[np.arange(y.size), y] = 1
#     return one_hot

# y_train = one_hot_encode(y_train)
# y_val = one_hot_encode(y_val)
# y_test = one_hot_encode(y_test)
# # Print to verify
# print("y_train shape:", y_train.shape)  # (54000, 10)
# print("y_val shape:", y_val.shape)      # (6000, 10)
# print("y_test shape:", y_test.shape)      # (10000, 10)

# # Define the Feedforward Neural Network class
# class NeuralNetwork:
#     def __init__(self, input_size=784, hidden_layers=[128, 64], output_size=10):
#         """
#         Initializes a fully connected neural network.
#         Parameters:
#             input_size (int): Number of input neurons (784 for Fashion-MNIST).
#             hidden_layers (list): List containing the number of neurons in each hidden layer.
#             output_size (int): Number of output neurons (10 for classification).
#         """
#         self.layers = [input_size] + hidden_layers + [output_size]  # Layers including input & output
#         self.weights = []
#         self.biases = []

#         # Initialize weights and biases
#         for i in range(len(self.layers) - 1):
#             self.weights.append(np.random.randn(self.layers[i], self.layers[i+1]) * 0.01)
#             self.biases.append(np.zeros((1, self.layers[i+1])))

#     def sigmoid(self, z):
#         z = np.clip(z, -500, 500)  # Prevent extreme values
#         return 1 / (1 + np.exp(-z))

#     def softmax(self, z):
#         exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Stability trick
#         return exp_z / np.sum(exp_z, axis=1, keepdims=True)

#     def forward(self, X):
#         """
#         Forward pass through the network.
#         Returns activations of all layers.
#         """
#         activations = [X]
#         for i in range(len(self.weights) - 1):
#             z = np.dot(activations[-1], self.weights[i]) + self.biases[i]
#             a = self.sigmoid(z)
#             activations.append(a)

#         # Output layer (softmax activation)
#         z_out = np.dot(activations[-1], self.weights[-1]) + self.biases[-1]
#         a_out = self.softmax(z_out)
#         activations.append(a_out)

#         return activations

#     def predict(self, X):
#         """
#         Predict class labels for given input.
#         """
#         output = self.forward(X)[-1]
#         return np.argmax(output, axis=1)


# # Create a neural network with a flexible architecture
# nn = NeuralNetwork(input_size=784, hidden_layers=[128, 64], output_size=10)

# # Forward pass example
# sample_input = x_train[:5]  # Take 5 sample images
# output_probs = nn.forward(sample_input)[-1]  # Get output probability distribution

# # Print predictions
# print("Predicted class probabilities:\n", output_probs)
# print("Predicted classes:", np.argmax(output_probs, axis=1))

import numpy as np
from keras.datasets import fashion_mnist

# Load Fashion-MNIST dataset
(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()

# Normalize and flatten images
x_train = x_train.reshape(x_train.shape[0], -1) / 255.0
x_test = x_test.reshape(x_test.shape[0], -1) / 255.0

# Create validation set (last 6000 samples from training set)
x_val = x_train[54000:]
y_val = y_train[54000:]

# Use the first 54,000 samples for training
x_train = x_train[:54000]
y_train = y_train[:54000]

# Convert labels to one-hot encoding
def one_hot_encode(y, num_classes=10):
    one_hot = np.zeros((y.size, num_classes))
    one_hot[np.arange(y.size), y] = 1
    return one_hot

y_train = one_hot_encode(y_train)
y_val = one_hot_encode(y_val)
y_test = one_hot_encode(y_test)

# ------------------------------ Neural Network Class --------------------------------------
class NeuralNetwork:
    def __init__(self, input_size=784, hidden_layers=[128, 64], output_size=10,
                 learning_rate=0.01, optimizer="sgd", weight_init="random",
                 activation="sigmoid", weight_decay=0.0):
        """
        Flexible Feedforward Neural Network with:
        - Weight Initialization: 'random' or 'xavier'
        - Activation Functions: 'sigmoid', 'tanh', 'relu'
        - Weight Decay (L2 Regularization): 0, 0.0005, 0.5
        """
        self.layers = [input_size] + hidden_layers + [output_size]
        self.learning_rate = learning_rate
        self.optimizer = optimizer
        self.weight_init = weight_init
        self.activation = activation
        self.weight_decay = weight_decay
        self.init_weights()

        # Optimizer-specific variables
        self.momentum = 0.9
        self.beta1 = 0.9  # Adam/Nadam
        self.beta2 = 0.999
        self.epsilon = 1e-8
        self.velocity = [np.zeros_like(w) for w in self.weights]
        self.squared_grads = [np.zeros_like(w) for w in self.weights]
        self.m = [np.zeros_like(w) for w in self.weights]
        self.v = [np.zeros_like(w) for w in self.weights]
        self.t = 0  # Time step

    def init_weights(self):
        """ Initialize weights and biases based on chosen method. """
        self.weights = []
        self.biases = []
        for i in range(len(self.layers) - 1):
            if self.weight_init == "random":
                self.weights.append(np.random.randn(self.layers[i], self.layers[i+1]) * 0.01)
            elif self.weight_init == "xavier":
                self.weights.append(np.random.randn(self.layers[i], self.layers[i+1]) * np.sqrt(1 / self.layers[i]))
            self.biases.append(np.zeros((1, self.layers[i+1])))

    def activation_function(self, z):
        """ Compute activation function based on user choice. """
        if self.activation == "sigmoid":
            return 1 / (1 + np.exp(-np.clip(z, -10, 10)))  # Prevent overflow
        elif self.activation == "tanh":
            return np.tanh(z)
        elif self.activation == "relu":
            return np.maximum(0, z)

    def activation_derivative(self, a):
        """ Compute derivative of activation function. """
        if self.activation == "sigmoid":
            return a * (1 - a)
        elif self.activation == "tanh":
            return 1 - a**2
        elif self.activation == "relu":
            return (a > 0).astype(float)

    def softmax(self, z):
        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))
        return exp_z / np.sum(exp_z, axis=1, keepdims=True)

    def forward(self, X):
        """ Forward pass through the network. """
        activations = [X]
        z_values = []

        for i in range(len(self.weights) - 1):
            z = np.dot(activations[-1], self.weights[i]) + self.biases[i]
            z_values.append(z)
            activations.append(self.activation_function(z))

        z_out = np.dot(activations[-1], self.weights[-1]) + self.biases[-1]
        z_values.append(z_out)
        activations.append(self.softmax(z_out))

        return activations, z_values

    def compute_loss(self, y_true, y_pred):
        """ Compute cross-entropy loss with optional L2 regularization. """
        loss = -np.sum(y_true * np.log(y_pred + self.epsilon)) / y_true.shape[0]
        l2_penalty = self.weight_decay * sum(np.sum(w**2) for w in self.weights) / 2
        return loss + l2_penalty

    def compute_accuracy(self, y_true, y_pred):
        """ Compute accuracy by comparing true vs predicted labels. """
        correct_predictions = np.sum(np.argmax(y_pred, axis=1) == np.argmax(y_true, axis=1))
        return correct_predictions / y_true.shape[0]

    def backward(self, X, y_true, activations, z_values):
        """ Backpropagation to compute gradients. """
        gradients_w = [np.zeros_like(w) for w in self.weights]
        gradients_b = [np.zeros_like(b) for b in self.biases]

        # Output layer gradient
        dL_dz = activations[-1] - y_true
        gradients_w[-1] = np.dot(activations[-2].T, dL_dz) + self.weight_decay * self.weights[-1]
        gradients_b[-1] = np.sum(dL_dz, axis=0, keepdims=True)

        # Hidden layers
        for i in reversed(range(len(self.weights) - 1)):
            dL_dz = np.dot(dL_dz, self.weights[i+1].T) * self.activation_derivative(activations[i+1])
            gradients_w[i] = np.dot(activations[i].T, dL_dz) + self.weight_decay * self.weights[i]
            gradients_b[i] = np.sum(dL_dz, axis=0, keepdims=True)

        return gradients_w, gradients_b

    def update_weights(self, gradients_w, gradients_b):
        """ Apply gradient updates using different optimizers. """
        self.t += 1  # Update time step for Adam/Nadam

        for i in range(len(self.weights)):
            if self.optimizer == "sgd":
                self.weights[i] -= self.learning_rate * gradients_w[i]
                self.biases[i] -= self.learning_rate * gradients_b[i]

            elif self.optimizer == "momentum":
                self.velocity[i] = self.momentum * self.velocity[i] - self.learning_rate * gradients_w[i]
                self.weights[i] += self.velocity[i]
                self.biases[i] -= self.learning_rate * gradients_b[i]

            elif self.optimizer == "nesterov":
                temp_weights = self.weights[i] + self.momentum * self.velocity[i]
                self.velocity[i] = self.momentum * self.velocity[i] - self.learning_rate * gradients_w[i]
                self.weights[i] = temp_weights + self.velocity[i]

            elif self.optimizer == "rmsprop":
                self.squared_grads[i] = 0.9 * self.squared_grads[i] + 0.1 * (gradients_w[i] ** 2)
                self.weights[i] -= self.learning_rate * gradients_w[i] / (np.sqrt(self.squared_grads[i]) + self.epsilon)

            elif self.optimizer == "adam":
                self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * gradients_w[i]
                self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (gradients_w[i] ** 2)
                m_hat = self.m[i] / (1 - self.beta1 ** self.t)
                v_hat = self.v[i] / (1 - self.beta2 ** self.t)
                self.weights[i] -= self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)

            elif self.optimizer == "nadam":
                m_hat = (self.beta1 * self.m[i] + (1 - self.beta1) * gradients_w[i]) / (1 - self.beta1 ** self.t)
                v_hat = self.v[i] / (1 - self.beta2 ** self.t)
                self.weights[i] -= self.learning_rate * (self.momentum * m_hat + (1 - self.momentum) * gradients_w[i]) / (np.sqrt(v_hat) + self.epsilon)

    def train(self, X_train, y_train, X_val, y_val, epochs=10, batch_size=32):
        """ Train the network using mini-batch gradient descent and track accuracy. """
        num_samples = X_train.shape[0]

        for epoch in range(epochs):
            # Shuffle training data
            indices = np.arange(num_samples)
            np.random.shuffle(indices)
            X_train, y_train = X_train[indices], y_train[indices]

            # Mini-batch training
            for i in range(0, num_samples, batch_size):
                X_batch = X_train[i:i + batch_size]
                y_batch = y_train[i:i + batch_size]

                # Forward pass
                activations, z_values = self.forward(X_batch)

                # Compute gradients
                gradients_w, gradients_b = self.backward(X_batch, y_batch, activations, z_values)

                # Update weights
                self.update_weights(gradients_w, gradients_b)

            # Compute training loss & accuracy
            train_activations, _ = self.forward(X_train)
            Train_loss = self.compute_loss(y_train, train_activations[-1])
            Train_accuracy = compute_accuracy(y_train, train_activations[-1])

            # Compute validation loss & accuracy
            val_activations, _ = self.forward(X_val)
            Val_loss = self.compute_loss(y_val, val_activations[-1])
            Val_accuracy = compute_accuracy(y_val, val_activations[-1])
            wandb.log({'Train_loss': Train_loss})
            wandb.log({'Train_accuracy': Train_accuracy })
            wandb.log({'epoch': epoch + 1})
            wandb.log({'Val_loss': Val_loss})
            wandb.log({'Val_accuracy': Val_accuracy })

            print(f"Epoch {epoch+1}: Train Loss = {Train_loss:.4f}, Train Acc = {Train_accuracy:.4f}, Val Loss = {Val_loss:.4f}, Val Acc = {Val_accuracy:.4f}")

def compute_accuracy(y_true, y_pred):
        """ Compute accuracy by comparing true vs predicted labels. """
        correct_predictions = np.sum(np.argmax(y_pred, axis=1) == np.argmax(y_true, axis=1))
        accuracy = correct_predictions / y_true.shape[0]
        return accuracy*100

# # Flatten both training and validation sets
# x_train = x_train.reshape(x_train.shape[0], -1)  # (54000, 784)
# x_val = x_val.reshape(x_val.shape[0], -1)  # (6000, 784)

# # Create Neural Network
# nn = NeuralNetwork(input_size=784, hidden_layers=[128, 64], output_size=10, learning_rate=0.01, optimizer="sgd", weight_init="xavier", activation="relu", weight_decay=0.0005)

# # Train the model and track accuracy
# nn.train(x_train, y_train, x_val, y_val, epochs=10, batch_size=32)

!pip install wandb

import wandb
import numpy as np
from types import SimpleNamespace
import random

key = input('Enter your API:')
wandb.login(key=key)

sweep_config = {
    'method': 'bayes',
    'name' : 'sweep cross entropy-3',
    'metric': {
      'name': 'Val_accuracy',
      'goal': 'maximize'
    },
    'parameters': {
        'epochs': {
            'values': [5,10]
        },
        'hidden_layers':{
            'values':[3,4,5]
        },
         'hidden_size':{
            'values':[32,64,128]
        },
        'weight_decay':{
            'values':[0, 0.0005, 0.5]
        },
        'learning_rate': {
            'values': [1e-3, 1e-4]
        },
        'optimizer': {
            'values': ['rmsprop', 'nadam','adam', 'nag','mgd','sgd']
        },
        'batch_size':{
            'values':[16,32,64]
        },
        'weight_init': {
            'values': ['xavier','random']
        },
        'activation': {
            'values': ['relu','tanh','sigmoid']
        },
    }
}

sweep_id = wandb.sweep(sweep=sweep_config, project='DA6401_Assignment1_ma23m011')

def main():
    with wandb.init() as run:
        run_name="-ac_"+wandb.config.activation+"-hs_"+str(wandb.config.hidden_size)+"-epc_"+str(wandb.config.epochs)+"-hl_"+str(wandb.config.hidden_layers)+"-regu_"+str(wandb.config.weight_decay)+"-eta_"+str(wandb.config.learning_rate)+"-optmz_"+wandb.config.optimizer+"-batch_"+str(wandb.config.batch_size)+"-wght_"+wandb.config.weight_init
        wandb.run.name=run_name
        nn = NeuralNetwork(input_size=784, hidden_layers=[wandb.config.hidden_size] * wandb.config.hidden_layers, output_size=10, learning_rate=wandb.config.learning_rate, optimizer=wandb.config.optimizer, weight_init=wandb.config.weight_init, activation=wandb.config.activation, weight_decay=wandb.config.weight_decay)
        nn.train(x_train, y_train, x_val, y_val, epochs=wandb.config.epochs, batch_size=wandb.config.batch_size)


# # Create Neural Network
# nn = NeuralNetwork(input_size=784, hidden_layers=[128, 64], output_size=10, learning_rate=0.01, optimizer="sgd")

# # Train the model and track accuracy
# nn.train(x_train, y_train, x_val, y_val, epochs=10, batch_size=32)

wandb.agent(sweep_id, function=main,count=100) # calls main function for count number of times.
wandb.finish()







